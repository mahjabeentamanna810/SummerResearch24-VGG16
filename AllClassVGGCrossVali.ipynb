{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def load_mitbih_dataset(path):\n",
    "    # Define a list of annotation symbols that represent different types of heartbeats\n",
    "    annots_list = ['N', 'L', 'R', 'e', 'j', 'S', 'A', 'a', 'J', 'V', 'E', 'F', '/', 'f', 'Q']\n",
    "\n",
    "    # Prepare empty lists to store signal segments and corresponding labels\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Create a dictionary to map each annotation symbol to a unique integer\n",
    "    annot_to_int = {annot: i for i, annot in enumerate(annots_list)}\n",
    "\n",
    "    # Read the list of ECG record filenames from the 'RECORDS' file in the dataset directory\n",
    "    record_list = []\n",
    "    with open(os.path.join(path, 'RECORDS'), 'r') as file:\n",
    "        record_list = [line.strip() for line in file]\n",
    "\n",
    "    # Create an instance of the MinMaxScaler to scale each segment to the range [0, 1]\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # Iterate over each record file in the dataset\n",
    "    for record_name in record_list:\n",
    "        # Load the ECG record and its annotations using the wfdb library\n",
    "        record = wfdb.rdrecord(os.path.join(path, record_name))\n",
    "        annotation = wfdb.rdann(os.path.join(path, record_name), 'atr')\n",
    "\n",
    "        # Extract the first channel of the ECG signal (assuming it's a single-channel ECG)\n",
    "        signal = record.p_signal[:, 0]\n",
    "        # Get the symbols and sample locations for each annotated beat\n",
    "        beat_annotations = annotation.symbol\n",
    "        beat_locations = annotation.sample\n",
    "\n",
    "        # Process each annotated beat in the current record\n",
    "        for sym, loc in zip(beat_annotations, beat_locations):\n",
    "            # Check if the annotation symbol is one of the types we're interested in\n",
    "            if sym in annots_list:\n",
    "                # Convert the annotation symbol to its corresponding integer label\n",
    "                label = annot_to_int[sym]\n",
    "                \n",
    "                # Define the window size around the beat location to extract the segment\n",
    "                win_size = 625  # This results in segments of 1250 samples centered on the beat\n",
    "                # Ensure that the window does not extend beyond the signal boundaries\n",
    "                if loc - win_size >= 0 and loc + win_size <= len(signal):\n",
    "                    # Extract the segment of the signal centered around the beat\n",
    "                    segment = signal[loc - win_size: loc + win_size]\n",
    "                    # Normalize the segment to the range [0, 1]\n",
    "                    segment = scaler.fit_transform(segment.reshape(-1, 1)).flatten()\n",
    "                    # Add the normalized segment to the list of segments\n",
    "                    X.append(segment)\n",
    "                    # Add the label to the list of labels\n",
    "                    y.append(label)\n",
    "\n",
    "    # Convert the lists of segments and labels to numpy arrays for use in machine learning models\n",
    "    X = np.array(X)\n",
    "    # Convert the integer labels to one-hot encoded format\n",
    "    y = to_categorical(y, num_classes=len(annots_list))\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape Function\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def reshape_ecg_to_2d(X, target_shape=(32, 32)):\n",
    "    \"\"\"\n",
    "    Reshape 1D ECG data into 2D format with three channels to fit into a VGG16 model.\n",
    "\n",
    "    Args:\n",
    "    X (array): 1D numpy array of ECG data.\n",
    "    target_shape (tuple): The target dimensions to which each ECG segment will be reshaped.\n",
    "\n",
    "    Returns:\n",
    "    numpy.array: A 4D array where each ECG segment is reshaped into 2D and replicated across three channels.\n",
    "    \"\"\"\n",
    "    # Initialize the scaler to normalize data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # Prepare a list to hold the reshaped segments\n",
    "    X_reshaped = []\n",
    "    \n",
    "    # Process each segment in the dataset\n",
    "    for segment in X:\n",
    "        # Normalize the segment\n",
    "        segment_normalized = scaler.fit_transform(segment.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Calculate the padding required to reach the target size, if necessary\n",
    "        padding = target_shape[0] * target_shape[1] - len(segment_normalized)\n",
    "        \n",
    "        if padding > 0:\n",
    "            # If the segment is too short, pad it with zeros at the end\n",
    "            segment_normalized = np.pad(segment_normalized, (0, padding), 'constant')\n",
    "        elif padding < 0:\n",
    "            # If the segment is too long, trim the excess\n",
    "            segment_normalized = segment_normalized[:padding]\n",
    "        \n",
    "        # Reshape the normalized segment into 2D format\n",
    "        segment_2d = segment_normalized.reshape(target_shape)\n",
    "        \n",
    "        # Stack the 2D image across three channels since VGG16 expects three-channel input images\n",
    "        segment_3ch = np.stack([segment_2d] * 3, axis=-1)\n",
    "        \n",
    "        # Append the reshaped segment to the list\n",
    "        X_reshaped.append(segment_3ch)\n",
    "\n",
    "    return np.array(X_reshaped)\n",
    "\n",
    "# Load your data\n",
    "X, y = load_mitbih_dataset(\"/home/researchgroup/mahjabeen_workspace/research/CLINet-ECG-Classification-2024/data/mit-bih/mitbih_database/\")\n",
    "\n",
    "# Reshape your 1D ECG data into 2D\n",
    "X_2d = reshape_ecg_to_2d(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1...\n",
      "Epoch 1/10\n",
      "6144/6148 [============================>.] - ETA: 0s - loss: 0.2199 - accuracy: 0.9429 - precision_4: 0.9567 - recall_4: 0.9283\n",
      "Epoch 1: val_loss improved from inf to 0.08759, saving model to ./model_fold_1.h5\n",
      "6148/6148 [==============================] - 59s 9ms/step - loss: 0.2198 - accuracy: 0.9429 - precision_4: 0.9567 - recall_4: 0.9283 - val_loss: 0.0876 - val_accuracy: 0.9766 - val_precision_4: 0.9808 - val_recall_4: 0.9728 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "6145/6148 [============================>.] - ETA: 0s - loss: 0.0921 - accuracy: 0.9764 - precision_4: 0.9798 - recall_4: 0.9730\n",
      "Epoch 2: val_loss improved from 0.08759 to 0.06754, saving model to ./model_fold_1.h5\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0922 - accuracy: 0.9763 - precision_4: 0.9797 - recall_4: 0.9730 - val_loss: 0.0675 - val_accuracy: 0.9847 - val_precision_4: 0.9859 - val_recall_4: 0.9833 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "6143/6148 [============================>.] - ETA: 0s - loss: 0.0806 - accuracy: 0.9795 - precision_4: 0.9821 - recall_4: 0.9773\n",
      "Epoch 3: val_loss improved from 0.06754 to 0.06164, saving model to ./model_fold_1.h5\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0806 - accuracy: 0.9795 - precision_4: 0.9821 - recall_4: 0.9773 - val_loss: 0.0616 - val_accuracy: 0.9844 - val_precision_4: 0.9869 - val_recall_4: 0.9824 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "6143/6148 [============================>.] - ETA: 0s - loss: 0.0772 - accuracy: 0.9810 - precision_4: 0.9835 - recall_4: 0.9786\n",
      "Epoch 4: val_loss did not improve from 0.06164\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0771 - accuracy: 0.9810 - precision_4: 0.9835 - recall_4: 0.9786 - val_loss: 0.0700 - val_accuracy: 0.9848 - val_precision_4: 0.9862 - val_recall_4: 0.9836 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "6148/6148 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9817 - precision_4: 0.9840 - recall_4: 0.9794\n",
      "Epoch 5: val_loss did not improve from 0.06164\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0741 - accuracy: 0.9817 - precision_4: 0.9840 - recall_4: 0.9794 - val_loss: 0.0641 - val_accuracy: 0.9856 - val_precision_4: 0.9880 - val_recall_4: 0.9833 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "6145/6148 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9822 - precision_4: 0.9844 - recall_4: 0.9798\n",
      "Epoch 6: val_loss did not improve from 0.06164\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0725 - accuracy: 0.9822 - precision_4: 0.9844 - recall_4: 0.9798 - val_loss: 0.0703 - val_accuracy: 0.9829 - val_precision_4: 0.9847 - val_recall_4: 0.9812 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "6145/6148 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9815 - precision_4: 0.9841 - recall_4: 0.9790\n",
      "Epoch 7: val_loss did not improve from 0.06164\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0756 - accuracy: 0.9815 - precision_4: 0.9841 - recall_4: 0.9791 - val_loss: 0.0671 - val_accuracy: 0.9883 - val_precision_4: 0.9890 - val_recall_4: 0.9861 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "6143/6148 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9824 - precision_4: 0.9851 - recall_4: 0.9803\n",
      "Epoch 8: val_loss did not improve from 0.06164\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0713 - accuracy: 0.9824 - precision_4: 0.9851 - recall_4: 0.9803 - val_loss: 0.0815 - val_accuracy: 0.9833 - val_precision_4: 0.9848 - val_recall_4: 0.9816 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "6148/6148 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9898 - precision_4: 0.9911 - recall_4: 0.9888\n",
      "Epoch 9: val_loss improved from 0.06164 to 0.04620, saving model to ./model_fold_1.h5\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0377 - accuracy: 0.9898 - precision_4: 0.9911 - recall_4: 0.9888 - val_loss: 0.0462 - val_accuracy: 0.9901 - val_precision_4: 0.9911 - val_recall_4: 0.9895 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "6145/6148 [============================>.] - ETA: 0s - loss: 0.0267 - accuracy: 0.9921 - precision_4: 0.9932 - recall_4: 0.9913\n",
      "Epoch 10: val_loss did not improve from 0.04620\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0267 - accuracy: 0.9921 - precision_4: 0.9932 - recall_4: 0.9913 - val_loss: 0.0486 - val_accuracy: 0.9905 - val_precision_4: 0.9912 - val_recall_4: 0.9904 - lr: 1.0000e-04\n",
      "Fold 1 completed in 573.04 seconds.\n",
      "342/342 [==============================] - 2s 5ms/step - loss: 0.0486 - accuracy: 0.9905 - precision_4: 0.9912 - recall_4: 0.9904\n",
      "Training fold 2...\n",
      "Epoch 1/10\n",
      "6143/6148 [============================>.] - ETA: 0s - loss: 0.2311 - accuracy: 0.9395 - precision: 0.9542 - recall: 0.9234\n",
      "Epoch 1: val_loss improved from inf to 0.09591, saving model to ./model_fold_2.h5\n",
      "6148/6148 [==============================] - 59s 9ms/step - loss: 0.2311 - accuracy: 0.9395 - precision: 0.9542 - recall: 0.9235 - val_loss: 0.0959 - val_accuracy: 0.9743 - val_precision: 0.9784 - val_recall: 0.9680 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "6146/6148 [============================>.] - ETA: 0s - loss: 0.0920 - accuracy: 0.9769 - precision: 0.9803 - recall: 0.9738\n",
      "Epoch 2: val_loss improved from 0.09591 to 0.08128, saving model to ./model_fold_2.h5\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0920 - accuracy: 0.9769 - precision: 0.9802 - recall: 0.9738 - val_loss: 0.0813 - val_accuracy: 0.9814 - val_precision: 0.9835 - val_recall: 0.9789 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "6144/6148 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9790 - precision: 0.9816 - recall: 0.9765\n",
      "Epoch 3: val_loss did not improve from 0.08128\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0827 - accuracy: 0.9790 - precision: 0.9816 - recall: 0.9764 - val_loss: 0.1252 - val_accuracy: 0.9687 - val_precision: 0.9707 - val_recall: 0.9664 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "6143/6148 [============================>.] - ETA: 0s - loss: 0.0771 - accuracy: 0.9811 - precision: 0.9838 - recall: 0.9785\n",
      "Epoch 4: val_loss improved from 0.08128 to 0.07364, saving model to ./model_fold_2.h5\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0770 - accuracy: 0.9811 - precision: 0.9838 - recall: 0.9785 - val_loss: 0.0736 - val_accuracy: 0.9833 - val_precision: 0.9858 - val_recall: 0.9810 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "6147/6148 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9820 - precision: 0.9845 - recall: 0.9795\n",
      "Epoch 5: val_loss did not improve from 0.07364\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0743 - accuracy: 0.9820 - precision: 0.9845 - recall: 0.9795 - val_loss: 0.1110 - val_accuracy: 0.9793 - val_precision: 0.9813 - val_recall: 0.9781 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "6143/6148 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9821 - precision: 0.9844 - recall: 0.9797\n",
      "Epoch 6: val_loss did not improve from 0.07364\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0739 - accuracy: 0.9821 - precision: 0.9844 - recall: 0.9797 - val_loss: 0.0852 - val_accuracy: 0.9812 - val_precision: 0.9829 - val_recall: 0.9804 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "6144/6148 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9824 - precision: 0.9848 - recall: 0.9805\n",
      "Epoch 7: val_loss did not improve from 0.07364\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0715 - accuracy: 0.9824 - precision: 0.9848 - recall: 0.9805 - val_loss: 0.0930 - val_accuracy: 0.9786 - val_precision: 0.9800 - val_recall: 0.9768 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "6148/6148 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9818 - precision: 0.9841 - recall: 0.9798\n",
      "Epoch 8: val_loss improved from 0.07364 to 0.07197, saving model to ./model_fold_2.h5\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0728 - accuracy: 0.9818 - precision: 0.9841 - recall: 0.9798 - val_loss: 0.0720 - val_accuracy: 0.9849 - val_precision: 0.9859 - val_recall: 0.9841 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "6148/6148 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9822 - precision: 0.9844 - recall: 0.9802\n",
      "Epoch 9: val_loss did not improve from 0.07197\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0741 - accuracy: 0.9822 - precision: 0.9844 - recall: 0.9802 - val_loss: 0.0916 - val_accuracy: 0.9832 - val_precision: 0.9840 - val_recall: 0.9829 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "6148/6148 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9826 - precision: 0.9850 - recall: 0.9805\n",
      "Epoch 10: val_loss did not improve from 0.07197\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0717 - accuracy: 0.9826 - precision: 0.9850 - recall: 0.9805 - val_loss: 0.1011 - val_accuracy: 0.9779 - val_precision: 0.9803 - val_recall: 0.9760 - lr: 0.0010\n",
      "Fold 2 completed in 575.30 seconds.\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 0.1011 - accuracy: 0.9779 - precision: 0.9803 - recall: 0.9760\n",
      "Training fold 3...\n",
      "Epoch 1/10\n",
      "6145/6148 [============================>.] - ETA: 0s - loss: 0.2083 - accuracy: 0.9451 - precision: 0.9591 - recall: 0.9315\n",
      "Epoch 1: val_loss improved from inf to 0.09294, saving model to ./model_fold_3.h5\n",
      "6148/6148 [==============================] - 59s 9ms/step - loss: 0.2082 - accuracy: 0.9451 - precision: 0.9592 - recall: 0.9315 - val_loss: 0.0929 - val_accuracy: 0.9755 - val_precision: 0.9779 - val_recall: 0.9736 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "6148/6148 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9759 - precision: 0.9795 - recall: 0.9724\n",
      "Epoch 2: val_loss improved from 0.09294 to 0.09250, saving model to ./model_fold_3.h5\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0936 - accuracy: 0.9759 - precision: 0.9795 - recall: 0.9724 - val_loss: 0.0925 - val_accuracy: 0.9775 - val_precision: 0.9837 - val_recall: 0.9686 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "6145/6148 [============================>.] - ETA: 0s - loss: 0.0802 - accuracy: 0.9790 - precision: 0.9820 - recall: 0.9765\n",
      "Epoch 3: val_loss improved from 0.09250 to 0.07814, saving model to ./model_fold_3.h5\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0802 - accuracy: 0.9790 - precision: 0.9820 - recall: 0.9765 - val_loss: 0.0781 - val_accuracy: 0.9791 - val_precision: 0.9824 - val_recall: 0.9747 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "6148/6148 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9808 - precision: 0.9838 - recall: 0.9782\n",
      "Epoch 4: val_loss did not improve from 0.07814\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0765 - accuracy: 0.9808 - precision: 0.9838 - recall: 0.9782 - val_loss: 0.0913 - val_accuracy: 0.9762 - val_precision: 0.9793 - val_recall: 0.9744 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "6148/6148 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9816 - precision: 0.9840 - recall: 0.9794\n",
      "Epoch 5: val_loss did not improve from 0.07814\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0757 - accuracy: 0.9816 - precision: 0.9840 - recall: 0.9794 - val_loss: 0.1016 - val_accuracy: 0.9801 - val_precision: 0.9820 - val_recall: 0.9781 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "6143/6148 [============================>.] - ETA: 0s - loss: 0.0710 - accuracy: 0.9825 - precision: 0.9849 - recall: 0.9805\n",
      "Epoch 6: val_loss did not improve from 0.07814\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0709 - accuracy: 0.9826 - precision: 0.9849 - recall: 0.9805 - val_loss: 0.0985 - val_accuracy: 0.9754 - val_precision: 0.9787 - val_recall: 0.9710 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "6146/6148 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9828 - precision: 0.9850 - recall: 0.9806\n",
      "Epoch 7: val_loss improved from 0.07814 to 0.06855, saving model to ./model_fold_3.h5\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0728 - accuracy: 0.9828 - precision: 0.9850 - recall: 0.9806 - val_loss: 0.0685 - val_accuracy: 0.9827 - val_precision: 0.9857 - val_recall: 0.9793 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "6143/6148 [============================>.] - ETA: 0s - loss: 0.0788 - accuracy: 0.9814 - precision: 0.9841 - recall: 0.9791\n",
      "Epoch 8: val_loss did not improve from 0.06855\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0787 - accuracy: 0.9814 - precision: 0.9841 - recall: 0.9791 - val_loss: 0.0687 - val_accuracy: 0.9841 - val_precision: 0.9861 - val_recall: 0.9829 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "6148/6148 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9822 - precision: 0.9846 - recall: 0.9800\n",
      "Epoch 9: val_loss did not improve from 0.06855\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0752 - accuracy: 0.9822 - precision: 0.9846 - recall: 0.9800 - val_loss: 0.0786 - val_accuracy: 0.9822 - val_precision: 0.9847 - val_recall: 0.9806 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "6147/6148 [============================>.] - ETA: 0s - loss: 0.0691 - accuracy: 0.9831 - precision: 0.9853 - recall: 0.9809\n",
      "Epoch 10: val_loss did not improve from 0.06855\n",
      "6148/6148 [==============================] - 57s 9ms/step - loss: 0.0691 - accuracy: 0.9831 - precision: 0.9853 - recall: 0.9809 - val_loss: 0.0757 - val_accuracy: 0.9830 - val_precision: 0.9850 - val_recall: 0.9801 - lr: 0.0010\n",
      "Fold 3 completed in 575.59 seconds.\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 0.0757 - accuracy: 0.9830 - precision: 0.9850 - recall: 0.9801\n",
      "Training fold 4...\n"
     ]
    }
   ],
   "source": [
    "#Cross-Validation Function\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import time\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_vgg16_model(num_classes, input_shape=(25, 50, 1), optimizer_type='adam', learning_rate=0.001):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', Precision(), Recall()])\n",
    "    return model\n",
    "\n",
    "def cross_validate_model(X, y, k=10, epochs=10, batch_size=4):\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold_no = 1\n",
    "    all_results = []\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    for train, test in kfold.split(X, y):\n",
    "        print(f'Training fold {fold_no}...')\n",
    "        fold_start_time = time.time()\n",
    "\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "\n",
    "        model = create_vgg16_model(num_classes=y_train.shape[1], input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]))\n",
    "\n",
    "        model_checkpoint_path = f'./model_fold_{fold_no}.h5'\n",
    "\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "            keras.callbacks.ModelCheckpoint(model_checkpoint_path, save_best_only=True, verbose=1),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "        ]\n",
    "\n",
    "        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=callbacks)\n",
    "\n",
    "        fold_end_time = time.time()\n",
    "        fold_elapsed_time = fold_end_time - fold_start_time\n",
    "        print(f'Fold {fold_no} completed in {fold_elapsed_time:.2f} seconds.')\n",
    "        results = model.evaluate(X_test, y_test, verbose=1)\n",
    "        all_results.append(results)\n",
    "\n",
    "        with open(f'cross_validation_results_fold_{fold_no}.json', 'w') as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "        fold_no += 1\n",
    "\n",
    "        # Clear memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    total_end_time = time.time()\n",
    "    total_elapsed_time = total_end_time - total_start_time\n",
    "    print(f'Total cross-validation time: {total_elapsed_time:.2f} seconds.')\n",
    "\n",
    "    return all_results, total_elapsed_time\n",
    "\n",
    "# Perform cross-validation\n",
    "results, total_time = cross_validate_model(X_2d, y, k=10, epochs=10, batch_size=16)\n",
    "\n",
    "# Save cross-validation results\n",
    "with open('cross_validation_results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print(\"Cross-validation completed. Results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation function\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "class_labels = ['N', 'L', 'R', 'e', 'j', 'S', 'A', 'a', 'J', 'V', 'E', 'F', '/', 'f', 'Q']\n",
    "\n",
    "def evaluate_model_on_folds(X, y, k=10):\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold_no = 1\n",
    "    all_confusion_matrices = []\n",
    "    all_metrics = []\n",
    "\n",
    "    for train, test in kfold.split(X, y):\n",
    "        model_path = f'./model_fold_{fold_no}.h5'\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f'Model file for fold {fold_no} not found!')\n",
    "            continue\n",
    "        \n",
    "        model = keras.models.load_model(model_path)\n",
    "        print(f'Model for fold {fold_no} loaded successfully.')\n",
    "\n",
    "        X_test = X[test]\n",
    "        y_test = y[test]\n",
    "\n",
    "        start_time = time.time()\n",
    "        results = model.evaluate(X_test, y_test, verbose=1)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f'Test Loss, Test Accuracy, and other metrics for fold {fold_no}: {results}')\n",
    "        print(f'Evaluation time for fold {fold_no}: {elapsed_time:.2f} seconds')\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "        unique_classes = np.unique(y_true_classes)\n",
    "\n",
    "        report_dict = classification_report(y_true_classes, y_pred_classes, labels=unique_classes, target_names=[class_labels[i] for i in unique_classes], output_dict=True)\n",
    "        print(f\"\\nClassification Report for fold {fold_no}:\")\n",
    "        print(classification_report(y_true_classes, y_pred_classes, labels=unique_classes, target_names=[class_labels[i] for i in unique_classes]))\n",
    "\n",
    "        cm = confusion_matrix(y_true_classes, y_pred_classes, labels=unique_classes)\n",
    "        all_confusion_matrices.append(cm)\n",
    "\n",
    "        metrics = precision_recall_fscore_support(y_true_classes, y_pred_classes, labels=unique_classes)\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[class_labels[i] for i in unique_classes], yticklabels=[class_labels[i] for i in unique_classes])\n",
    "        plt.title(f'Confusion Matrix for Fold {fold_no}')\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.ylabel('True Labels')\n",
    "        plt.show()\n",
    "\n",
    "        fold_no += 1\n",
    "\n",
    "        # Clear memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    return all_confusion_matrices, all_metrics\n",
    "\n",
    "def calculate_average_metrics(all_metrics):\n",
    "    avg_precision = []\n",
    "    avg_recall = []\n",
    "    avg_fscore = []\n",
    "    avg_support = []\n",
    "\n",
    "    for metrics in all_metrics:\n",
    "        precision, recall, fscore, support = metrics\n",
    "        avg_precision.append(np.mean(precision))\n",
    "        avg_recall.append(np.mean(recall))\n",
    "        avg_fscore.append(np.mean(fscore))\n",
    "        avg_support.append(np.mean(support))\n",
    "\n",
    "    return {\n",
    "        \"avg_precision\": np.mean(avg_precision),\n",
    "        \"avg_recall\": np.mean(avg_recall),\n",
    "        \"avg_fscore\": np.mean(avg_fscore),\n",
    "        \"avg_support\": np.mean(avg_support)\n",
    "    }\n",
    "\n",
    "# Evaluate model on each fold\n",
    "all_confusion_matrices, all_metrics = evaluate_model_on_folds(X_2d, y, k=10)\n",
    "\n",
    "# Calculate and print average metrics\n",
    "average_metrics = calculate_average_metrics(all_metrics)\n",
    "\n",
    "print(\"\\nAverage metrics across all folds:\")\n",
    "print(f\"Precision: {average_metrics['avg_precision']}\")\n",
    "print(f\"Recall: {average_metrics['avg_recall']}\")\n",
    "print(f\"F1-Score: {average_metrics['avg_fscore']}\")\n",
    "print(f\"Support: {average_metrics['avg_support']}\")\n",
    "\n",
    "# Visualize all confusion matrices in one figure\n",
    "fig, axes = plt.subplots(5, 2, figsize=(20, 25))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    sns.heatmap(all_confusion_matrices[i], annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels, ax=axes[i])\n",
    "    axes[i].set_title(f'Confusion Matrix for Fold {i+1}')\n",
    "    axes[i].set_xlabel('Predicted Labels')\n",
    "    axes[i].set_ylabel('True Labels')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newtest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
